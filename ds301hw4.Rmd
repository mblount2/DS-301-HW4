---
title: "Appendix"
author: "Mikaela Blount"
date: "3/5/2020"
output: html_document
---

Problem 1:
```{r}
insurance = read.csv("~/Downloads/insurance.csv")
```

a)
```{r}
str(insurance)
```
All predictors are stored correctly.

b)
```{r}
fit = lm(charges ~ age+bmi+gender+gender:age+gender:bmi, data = insurance)
summary(fit)
```

c)
Males: predicted charges = -8012.791 + 238.632(age) + 409.869(bmi)
Females: predicted charges = -4515.219 + 246.919(age) + 241.319(bmi)

d)
```{r}
library(dplyr)

male = insurance %>%
  filter(gender == "male")

fit_males = lm(charges ~ age+bmi, data = male)
summary(fit_males)

female = insurance %>%
  filter(gender == "female")

fit_females = lm(charges ~ age+bmi, data = female)
summary(fit_females)

```

e)
They are the same regression coefficent estimates indicating that predicted charges will be the same.

f)
It is not possible because in the subsetted data, you lose gender as a predictor. When gender is loss as a predictor, you cannot test whether it is significant or not because it is no longer in the model.

g)
The biggest advantage in using the model from part (b) is the simplicity of interpretting the model. The model from part (b) can be used for both males and females and to see how gender affects the mean of Y with one equation. The classmate's approach will give you the same results but it involves two seperate equations for the two genders.

h) 
At least one of the predictors is correlated with the response variable, but they do not explain much of the variability in the response variable. R-square measures the variability around the regression line so having a significant F-test statistic indicates at least one of the predictors is a significant predictor of health care charges but there is much variability around the regression line.

i)
```{r}
names(insurance)

predCharges = data.frame(age = 10, gender = "male", bmi = 13)

predict(fit, predCharges)
```

j)
```{r}
summary(insurance$age)
summary(insurance$bmi)
```

Since the individual in part (i) is outside the ranges for both age and bmi, there will be a problem with extrapolation. Extrapolation is volatile because we do not have data to support the prediction because in this case, the individual is 5 years younger and has a smaller bmi than anyone in the dataset. This is especially a problem in this model because health care charges will never be less than zero (the hospital will never owe the patient money for health care procedures), but the results show the hospital owes the individual -$298.16.   


Problem 2:
```{r}
saHeart <- read.delim2("~/Downloads/SAheart.txt",header = TRUE, sep = ",")
saHeart$ldl = as.numeric(saHeart$ldl)
saHeart$tobacco = as.numeric(saHeart$tobacco)
str(saHeart)
```


a)
```{r}
logit.fit = glm(chd~tobacco+ldl+famhist+age, data = saHeart)
summary(logit.fit)
```

b)
```{r}
D_star = logit.fit$null.deviance - logit.fit$deviance

# compute p-value
p = 4
df = p - 1
pchisq(D_star,df,lower.tail=FALSE)
```

Resisdual deviance: 82.89  on 457  degrees of freedom
Null deviance: 104.59  on 461  degrees of freedom

H0: B1 = B2 = B3 = B4 = 0
HA: At least one of the Bj's is not equal to 0

Test Statistic: D0 - D = 104.59 - 82.89 = 21.7

Null Distribution: Chi-square distribution with 3 degrees of freedom

We reject the null hypothesis in favor of the alternative hypothesis that chi-square does not equal zero because our p-value of 7.537321e-05 is smaller than any reasonable alpha.

c)
```{r}
head(predict(logit.fit),n = 10)
```

d)
```{r}
p_hat = predict(logit.fit, type = "response")

n = dim(saHeart)[1]
pred_labels = rep(0,n)
pred_labels[p_hat > 0.5] = 1

table1 = table(pred_labels,saHeart$chd) 
misRate = 1-mean(pred_labels == saHeart$chd)
corRate = 1-misRate


true_neg = table1[1,1]/sum(table1[,1])

true_pos = table1[2,2]/sum(table1[,2])

false_neg = table1[1,2]/sum(table1[,2])

false_pos = table1[2,1]/sum(table1[,1])

table1
corRate
true_neg
true_pos
misRate
false_neg
false_pos
```
Correct classification: 0.7294372
True neg: 0.8543046
True pos: 0.49375
Misclassifaction rate: 0.2705628
False neg: 0.50625
False pos: 0.1456954

e)
```{r}
set.seed(3)

n2 = dim(saHeart)[1]
index = sample(1:n2,n2/2,replace = FALSE)

train_saHeart = saHeart[index,]
test_saHeart = saHeart[-index,]

logit.train = glm(chd~ldl+famhist+age, data = train_saHeart)
summary(logit.train)
```


f)
```{r}
p_hat_test = predict(logit.train,test_saHeart,type = "response")

n2 = dim(test_saHeart)[1]
pred_labels2 = rep(0,n2)
pred_labels2[p_hat_test > 0.5] = 1

table2 = table(pred_labels2,test_saHeart$chd) 
table2

1-mean(pred_labels2 == test_saHeart$chd)
```

The overall misclassification rate is 32.46753%. This is a slightly higher misclassification rate compared to part (d) of 27.05628% but only by 5.41125% difference.  






















